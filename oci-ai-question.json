[
    {
        "question": "Which statement is true about Fine-tuning and Parameter-Efficient Fine-Tuning (PEFT)?",
        "options": [
            "Fine-tuning and PEFT do not involve model modification; they differ only in the type of data used for training, with Fine-tuning requiring labeled data and PEFT using unlabeled data.",
            "Fine-tuning requires training the entire model on new data, often leading to substantial computational costs, whereas PEFT involves updating only a small subset of parameters, minimizing computational requirements and data needs.",
            "PEFT requires replacing the entire model architecture with a new one designed specifically for the new task, making it significantly more data-intensive than Fine-tuning.",
            "Both Fine-tuning and PEFT require the model to be trained from scratch on new data, making them equally data and computationally intensive."
        ],
        "answer": [1],
        "explanation": "Fine-tuning requires training the entire model on new data, which is computationally expensive. PEFT updates only a small subset of parameters, reducing computational requirements."
    },
    {
        "question": "What does a cosine distance of 0 indicate about the relationship between two embeddings?",
        "options": [
            "They are completely dissimilar.",
            "They are unrelated.",
            "They have the same magnitude.",
            "They are similar in direction."
        ],
        "answer": [3],
        "explanation": "A cosine distance of 0 indicates that the two embeddings are similar in direction, meaning they point in the same direction in the vector space."
    },
    {
        "question": "What does the RAG Sequence model do in the context of generating a response?",
        "options": [
            "It retrieves relevant documents only for the initial part of the query and ignores the rest.",
            "It modifies the input query before retrieving relevant documents to ensure a diverse response.",
            "It retrieves a single relevant document for the entire input query and generates a response based on that alone.",
            "For each input query, it retrieves a set of relevant documents and considers them together to generate a cohesive response."
        ],
        "answer": [3],
        "explanation": "RAG Sequence retrieves a set of relevant documents for each input query and considers them together to generate a cohesive response."
    },
    {
        "question": "Given the following code block: history = StreamlitChatMessageHistory(key=\"chat_messages\") memory = ConversationBufferMemory(chat_memory=history) Which statement is NOT true about StreamlitChatMessageHistory?",
        "options": [
            "A given StreamlitChatMessageHistory will NOT be persisted.",
            "StreamlitChatMessageHistory will store messages in Streamlit session state at the specified key.",
            "StreamlitChatMessageHistory can be used in any type of LLM application.",
            "A given StreamlitChatMessageHistory will not be shared across user sessions."
        ],
        "answer": [2],
        "explanation": "StreamlitChatMessageHistory can be used in any type of LLM application, which is true. The statement 'A given StreamlitChatMessageHistory will NOT be persisted' is not true."
    },
    {
        "question": "What does accuracy measure in the context of fine-tuning results for a generative model?",
        "options": [
            "The proportion of incorrect predictions made by the model during an evaluation",
            "The number of predictions a model makes, regardless of whether they are correct or incorrect",
            "How many predictions the model made correctly out of all the predictions in an evaluation",
            "The depth of the neural network layers used in the model"
        ],
        "answer": [2],
        "explanation": "Accuracy measures how many predictions the model made correctly out of all the predictions in an evaluation."
    },
    {
        "question": "How can the concept of 'Groundedness' differ from 'Answer Relevance' in the context of Retrieval Augmented Generation (RAG)?",
        "options": [
            "Groundedness measures relevance to the user query, whereas Answer Relevance evaluates data integrity.",
            "Groundedness pertains to factual correctness, whereas Answer Relevance concerns query relevance.",
            "Groundedness refers to contextual alignment, whereas Answer Relevance deals with syntactic accuracy.",
            "Groundedness focuses on data integrity, whereas Answer Relevance emphasizes lexical diversity."
        ],
        "answer": [1],
        "explanation": "Groundedness pertains to factual correctness, whereas Answer Relevance concerns query relevance."
    },
    {
        "question": "What do prompt templates use for templating in language model applications?",
        "options": [
            "Python's lambda functions",
            "Python's class and object structures",
            "Python's str.format syntax",
            "Python's list comprehension syntax"
        ],
        "answer": [2],
        "explanation": "Prompt templates use Python's str.format syntax for templating in language model applications."
    },
    {
        "question": "When is fine-tuning an appropriate method for customizing a Large Language Model (LLM)?",
        "options": [
            "When the LLM does not perform well on a task and the data for prompt engineering is too large",
            "When the LLM already understands the topics necessary for text generation",
            "When the LLM requires access to the latest data for generating outputs",
            "When you want to optimize the model without any instructions"
        ],
        "answer": [0],
        "explanation": "Fine-tuning is appropriate when the LLM does not perform well on a task and the data for prompt engineering is too large."
    },
    {
        "question": "How does a presence penalty function in language model generation?",
        "options": [
            "It penalizes a token each time it appears after the first occurrence.",
            "It applies a penalty only if the token has appeared more than twice.",
            "It penalizes all tokens equally, regardless of how often they have appeared.",
            "It penalizes only tokens that have never appeared in the text before."
        ],
        "answer": [0],
        "explanation": "A presence penalty penalizes a token each time it appears after the first occurrence."
    },
    {
        "question": "Which is a characteristic of T-Few fine-tuning for Large Language Models (LLMs)?",
        "options": [
            "It increases the training time as compared to Vanilla fine-tuning.",
            "It updates all the weights of the model uniformly.",
            "It does not update any weights but restructures the model architecture.",
            "It selectively updates only a fraction of the model's weights."
        ],
        "answer": [3],
        "explanation": "T-Few fine-tuning selectively updates only a fraction of the model's weights."
    },
    {
        "question": "How are documents usually evaluated in the simplest form of keyword-based search?",
        "options": [
            "According to the length of the documents",
            "Based on the presence and frequency of the user-provided keywords",
            "By the complexity of language used in the documents",
            "Based on the number of images and videos contained in the documents"
        ],
        "answer": [1],
        "explanation": "Documents are evaluated based on the presence and frequency of the user-provided keywords in the simplest form of keyword-based search."
    },
    {
        "question": "What is the purpose of Retrievers in LangChain?",
        "options": [
            "To train Large Language Models",
            "To retrieve relevant information from knowledge bases",
            "To break down complex tasks into smaller steps",
            "To combine multiple components into a single pipeline"
        ],
        "answer": [1],
        "explanation": "Retrievers in LangChain are used to retrieve relevant information from knowledge bases."
    },
    {
        "question": "Which LangChain component is responsible for generating the linguistic output in a chatbot system?",
        "options": [
            "LLMs",
            "Document Loaders",
            "LangChain Application",
            "Vector Stores"
        ],
        "answer": [0],
        "explanation": "Large Language Models (LLMs) are responsible for generating the linguistic output in a chatbot system."
    },
    {
        "question": "In which scenario is soft prompting appropriate compared to other training styles?",
        "options": [
            "When the model requires continued pretraining on unlabeled data",
            "When there is a significant amount of labeled, task-specific data available",
            "When the model needs to be adapted to perform well in a domain on which it was not originally trained",
            "When there is a need to add learnable parameters to a Large Language Model (LLM) without task-specific training"
        ],
        "answer": [3],
        "explanation": "Soft prompting is appropriate when there is a need to add learnable parameters to a Large Language Model (LLM) without task-specific training."
    },
    {
        "question": "In the context of generating text with a Large Language Model (LLM), what does the process of greedy decoding entail?",
        "options": [
            "Choosing the word with the highest probability at each step of decoding",
            "Selecting a random word from the entire vocabulary at each step",
            "Picking a word based on its position in a sentence structure",
            "Using a weighted random selection based on a modulated distribution"
        ],
        "answer": [0],
        "explanation": "Greedy decoding involves choosing the word with the highest probability at each step of decoding."
    },
    {
        "question": "Accuracy in vector databases contributes to the effectiveness of Large Language Models (LLMs) by preserving a specific type of relationship. What is the nature of these relationships, and why are they crucial for language models?",
        "options": [
            "Temporal relationships; necessary for predicting future linguistic trends",
            "Hierarchical relationships; important for structuring database queries",
            "Semantic relationships; crucial for understanding context and generating precise language",
            "Linear relationships; they simplify the modeling process"
        ],
        "answer": [2],
        "explanation": "Semantic relationships in vector databases are crucial for understanding context and generating precise language, making them essential for the effectiveness of Large Language Models (LLMs)."
    },
    {
        "question": "In the simplified workflow for managing and querying vector data, what is the role of indexing?",
        "options": [
            "To convert vectors into a nonindexed format for easier retrieval",
            "To categorize vectors based on their originating data type (text, images, audio)",
            "To compress vector data for minimized storage usage",
            "To map vectors to a data structure for faster searching, enabling efficient retrieval"
        ],
        "answer": [3],
        "explanation": "Indexing maps vectors to a data structure for faster searching, enabling efficient retrieval."
    },
    {
        "question": "What does the Loss metric indicate about a model's predictions?",
        "options": [
            "Loss measures the total number of predictions made by a model.",
            "Loss is a measure that indicates how wrong the model's predictions are.",
            "Loss indicates how good a prediction is, and it should increase as the model improves.",
            "Loss describes the accuracy of the right predictions rather than the incorrect ones."
        ],
        "answer": [1],
        "explanation": "Loss is a measure that indicates how wrong the model's predictions are."
    },
    {
        "question": "What is the purpose of Retrieval Augmented Generation (RAG) in text generation?",
        "options": [
            "To retrieve text from an external source and present it without any modifications",
            "To generate text using extra information obtained from an external data source",
            "To store text in an external database without using it for generation",
            "To generate text based only on the model's internal knowledge without external data"
        ],
        "answer": [1],
        "explanation": "RAG generates text using extra information obtained from an external data source."
    },
    {
        "question": "How does the temperature setting in a decoding algorithm influence the probability distribution over the vocabulary?",
        "options": [
            "Increasing the temperature flattens the distribution, allowing for more varied word choices.",
            "Temperature has no effect on probability distribution; it only changes the speed of decoding.",
            "Increasing the temperature removes the impact of the most likely word.",
            "Decreasing the temperature broadens the distribution, making less likely words more probable."
        ],
        "answer": [0],
        "explanation": "Increasing the temperature flattens the distribution, allowing for more varied word choices."
    },
    {
        "question": "How does the structure of vector databases differ from traditional relational databases?",
        "options": [
            "It uses simple row-based data storage.",
            "A vector database stores data in a linear or tabular format.",
            "It is based on distances and similarities in a vector space.",
            "It is not optimized for high-dimensional spaces."
        ],
        "answer": [2],
        "explanation": "Vector databases are based on distances and similarities in a vector space, which is different from the row-based storage of traditional relational databases."
    },
    {
        "question": "Which statement is true about string prompt templates and their capability regarding variables?",
        "options": [
            "They support any number of variables, including the possibility of having none.",
            "They require a minimum of two variables to function properly.",
            "They are unable to use any variables.",
            "They can only support a single variable at a time."
        ],
        "answer": [0],
        "explanation": "String prompt templates support any number of variables, including the possibility of having none."
    }
]
