[
    {
        "question": "A company needs to ensure secure access to its AWS resources across multiple accounts. They want to implement a role-based access control strategy and manage permissions efficiently. Which services and practices should they use? (Select TWO.)",
        "options": [
            "AWS Identity and Access Management (IAM)",
            "Amazon GuardDuty",
            "AWS Single Sign-On (AWS SSO)",
            "AWS Secrets Manager",
            "Amazon Macie"
        ],
        "answer": [0, 2],
        "explanation": "IAM is essential for managing access to AWS services and resources securely. AWS SSO is used to manage SSO access and user permissions across multiple AWS accounts centrally."
    },
    {
        "question": "A company is designing a VPC architecture with private and public subnets. The EC2 instances in the private subnet need to connect to the internet to download software patches but should not be directly accessible from the internet. What should be done to allow the EC2 instances to download patches securely? (Select TWO.)",
        "options": [
            "Assign Elastic IP addresses to the EC2 instances.",
            "Configure a NAT gateway in a public subnet.",
            "Define a custom route table with a route to the NAT gateway for internet traffic and associate it with the private subnets.",
            "Configure a VPN connection.",
            "Use an internet gateway in the private subnet."
        ],
        "answer": [1, 2],
        "explanation": "A NAT gateway allows instances in a private subnet to connect to the internet or other AWS services. The custom route table ensures that the traffic from the instances in the private subnets is routed through the NAT gateway for internet access."
    },
    {
        "question": "A company needs to ensure that their data stored in Amazon S3 is encrypted at rest using encryption keys stored on-premises. Which of the following options meet this requirement? (Select TWO.)",
        "options": [
            "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).",
            "Use server-side encryption with AWS KMS managed encryption keys (SSE-KMS).",
            "Use server-side encryption with customer-provided encryption keys (SSE-C).",
            "Use client-side encryption to provide at-rest encryption.",
            "Use an AWS Lambda function invoked by Amazon S3 events to encrypt the data using the customerâ€™s keys."
        ],
        "answer": [2, 3],
        "explanation": "SSE-C allows you to manage your encryption keys on-premises and provide them to AWS for encrypting data at rest. With client-side encryption, data is encrypted by the client before it is uploaded to Amazon S3. The keys are managed and stored on-premises."
    },
    {
        "question": "A company wants to design a highly available and fault-tolerant architecture for its web application. The application runs on EC2 instances in a single Availability Zone (AZ). Which steps should a solutions architect take to provide high availability? (Select TWO.)",
        "options": [
            "Create new public and private subnets in the same AZ.",
            "Create an Amazon EC2 Auto Scaling group and Application Load Balancer spanning multiple AZs for the web application instances.",
            "Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer.",
            "Create new public and private subnets in a new AZ. Create a database using an EC2 instance in the public subnet in the new AZ. Migrate the old database contents to the new database.",
            "Create new public and private subnets in the same VPC, each in a new AZ. Create an Amazon RDS Multi-AZ DB instance in the private subnets. Migrate the old database contents to the new DB instance."
        ],
        "answer": [1, 2],
        "explanation": "Creating an Auto Scaling group and Application Load Balancer spanning multiple AZs ensures that the web application instances are distributed across multiple Availability Zones, providing high availability and fault tolerance. Adding the existing web application instances to an Auto Scaling group behind an Application Load Balancer further enhances high availability by automatically scaling the number of instances based on demand and distributing incoming traffic evenly."
    },
    {
        "question": "A company runs a public-facing three-tier web application in a VPC across multiple Availability Zones. The application tier EC2 instances running in private subnets need to download software patches from the internet. The instances cannot be directly accessible from the internet. Which actions should be taken to allow the EC2 instances to download the needed patches? (Select TWO.)",
        "options": [
            "Assign Elastic IP addresses to the EC2 instances.",
            "Configure a NAT gateway in a public subnet.",
            "Define a custom route table with a route to the NAT gateway for internet traffic and associate it with the private subnets for the application tier.",
            "Configure a VPN connection.",
            "Use an internet gateway in the private subnet."
        ],
        "answer": [1, 2],
        "explanation": "A NAT gateway allows instances in a private subnet to connect to the internet or other AWS services while preventing the internet from initiating a connection with those instances. The custom route table ensures that the traffic from the instances in the private subnets is routed through the NAT gateway for internet access."
    },
    {
        "question": "A company needs to ensure secure workloads and applications by controlling ports, protocols, and network traffic on AWS. Which services and features should they use? (Select TWO.)",
        "options": [
            "Security Groups",
            "Amazon CloudFront",
            "Network ACLs",
            "AWS Direct Connect",
            "Amazon S3"
        ],
        "answer": [0, 2],
        "explanation": "Security Groups act as virtual firewalls for your instances to control inbound and outbound traffic. They operate at the instance level. Network ACLs provide an additional layer of security that acts at the subnet level, controlling traffic to and from one or more subnets."
    },
    {
        "question": "A company wants to design a resilient architecture that automatically scales based on traffic. The company is using a web application hosted on Amazon EC2 instances. What should the solutions architect recommend to meet these requirements? (Select TWO.)",
        "options": [
            "Use Amazon EC2 Auto Scaling.",
            "Use AWS CloudTrail to monitor scaling events.",
            "Use an Application Load Balancer (ALB).",
            "Use Amazon CloudWatch for logging.",
            "Use AWS Config to track configuration changes."
        ],
        "answer": [0, 2],
        "explanation": "Auto Scaling ensures that the number of Amazon EC2 instances increases or decreases automatically based on traffic demands, providing resilience and scalability. An ALB distributes incoming application traffic across multiple targets, such as EC2 instances, to ensure high availability and fault tolerance."
    },
    {
        "question": "A company is using Amazon EC2 Reserved Instances to run its data processing workload. The nightly job typically takes 7 hours to run and must finish within a 10-hour time window. The company anticipates temporary increases in demand at the end of each month that will cause the job to run over the time limit with the capacity of the current resources. Once started, the processing job cannot be interrupted before completion. The company wants to implement a solution that would provide increased resource capacity as cost-effectively as possible. What should a solutions architect do to accomplish this?",
        "options": [
            "Deploy On-Demand Instances during periods of high demand.",
            "Create a second EC2 reservation for additional instances.",
            "Deploy Spot Instances during periods of high demand.",
            "Increase the EC2 instance size in the EC2 reservation to support the increased workload."
        ],
        "answer": [0],
        "explanation": "On-Demand Instances allow you to pay for compute capacity by the hour or second with no long-term commitments. They are suitable for workloads that cannot be interrupted and need to scale up temporarily to meet demand spikes, providing a flexible and cost-effective solution."
    },
    {
        "question": "A company runs an online voting system for a weekly live television program. During broadcasts, users submit hundreds of thousands of votes within minutes to a front-end fleet of Amazon EC2 instances that run in an Auto Scaling group. The EC2 instances write the votes to an Amazon RDS database. However, the database is unable to keep up with the requests that come from the EC2 instances. A solutions architect must design a solution that processes the votes in the most efficient manner and without downtime. What should the solutions architect do to meet these requirements?",
        "options": [
            "Migrate the front-end application to AWS Lambda. Use Amazon API Gateway to route user requests to the Lambda functions.",
            "Scale the database horizontally by converting it to a Multi-AZ deployment. Configure the front-end application to write to both the primary and secondary DB instances.",
            "Configure the front-end application to send votes to an Amazon Simple Queue Service (Amazon SQS) queue. Provision worker instances to read the SQS queue and write the vote information to the database.",
            "Use Amazon EventBridge (Amazon CloudWatch Events) to create a scheduled event to re-provision the database with larger, memory optimized instances during voting periods. When voting ends, re-provision the database to use smaller instances."
        ],
        "answer": [2],
        "explanation": "This approach decouples the ingestion of votes from the database, allowing the voting system to continue processing votes without waiting for the database writes. Worker instances can process votes from the SQS queue at a rate the database can handle, ensuring no votes are lost and the system remains responsive."
    },
    {
        "question": "An application running on AWS uses an Amazon Aurora Multi-AZ DB cluster deployment for its database. When evaluating performance metrics, a solutions architect discovered that the database reads are causing high I/O and adding latency to the write requests against the database. What should the solutions architect do to separate the read requests from the write requests?",
        "options": [
            "Enable read-through caching on the Aurora database.",
            "Update the application to read from the Multi-AZ standby instance.",
            "Create an Aurora read replica and modify the application to use the appropriate endpoints.",
            "Create a second Aurora database and link it to the primary database as a read replica."
        ],
        "answer": [2],
        "explanation": "Creating an Aurora read replica provides a way to offload read traffic from the primary instance, reducing the read I/O load on the primary database and improving the overall performance by separating read and write operations. Aurora replicas share the same underlying storage as the main database, ensuring minimal lag."
    },
    {
        "question": "A company is designing a high-performing and scalable network architecture. They need to ensure that their network can scale to accommodate future needs and provide low latency for end users. Which services and features should they use? (Select TWO.)",
        "options": [
            "Amazon CloudFront",
            "AWS Direct Connect",
            "AWS Transit Gateway",
            "AWS Global Accelerator",
            "AWS VPN"
        ],
        "answer": [0, 3],
        "explanation": "Amazon CloudFront is a content delivery network (CDN) that provides low latency and high transfer speeds by caching content at edge locations around the world. AWS Global Accelerator improves the availability and performance of applications with users globally by routing traffic through the AWS global network and optimizing the path to your application."
    },
    {
        "question": "A company is designing cost-optimized storage solutions for its data. They need to manage data lifecycle policies and ensure cost-effectiveness. Which AWS services and features should they use? (Select TWO.)",
        "options": [
            "Amazon S3 Lifecycle Policies",
            "AWS Snowball",
            "Amazon S3 Glacier",
            "Amazon FSx",
            "Amazon RDS"
        ],
        "answer": [0, 2],
        "explanation": "S3 Lifecycle Policies allow you to automate the transition of objects to different storage classes, such as moving data to cheaper storage options like S3 Glacier or deleting data after a specified period. S3 Glacier is a low-cost storage service designed for data archiving and long-term backup. It offers cost-effective storage for infrequently accessed data."
    },
    {
        "question": "A company is experiencing high I/O and latency issues with its Amazon Aurora database due to read operations. They need to improve read performance without impacting write operations. What should the solutions architect recommend?",
        "options": [
            "Enable read-through caching on the Aurora database.",
            "Update the application to read from the Multi-AZ standby instance.",
            "Create an Aurora read replica and modify the application to use the appropriate endpoints.",
            "Create a second Aurora database and link it to the primary database as a read replica."
        ],
        "answer": [2],
        "explanation": "Creating an Aurora read replica provides a way to offload read traffic from the primary instance, reducing the read I/O load on the primary database and improving the overall performance by separating read and write operations. Aurora replicas share the same underlying storage as the main database, ensuring minimal lag."
    },
    {
        "question": "A company needs to design a secure and resilient architecture for its critical applications. The applications require multi-region failover capability and secure access to AWS resources. Which of the following solutions should the solutions architect implement? (Select TWO.)",
        "options": [
            "Use Amazon Route 53 for DNS failover across multiple regions.",
            "Use AWS Direct Connect for secure access to AWS resources.",
            "Implement AWS IAM roles and policies for access management.",
            "Use Amazon CloudFront for content delivery across multiple regions.",
            "Deploy the applications using AWS Elastic Beanstalk in multiple regions."
        ],
        "answer": [0, 2],
        "explanation": "Amazon Route 53 is a highly available and scalable DNS web service that can route end-user requests to endpoints in multiple regions, providing multi-region failover capabilities. AWS IAM roles and policies are essential for securely managing access to AWS resources. They help ensure that users and applications have the necessary permissions while adhering to the principle of least privilege."
    },
    {
        "question": "A company needs to design a high-performing and scalable database solution. The database will handle a high volume of read and write operations and must scale seamlessly. Which AWS service should the solutions architect recommend?",
        "options": [
            "Amazon RDS for MySQL",
            "Amazon DynamoDB",
            "Amazon Aurora",
            "Amazon Redshift"
        ],
        "answer": [1],
        "explanation": "DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to handle high volumes of read and write operations, making it suitable for applications that require low-latency data access at any scale."
    },
        {
            "question": "A company needs to implement a secure authentication mechanism for its AWS resources that span multiple accounts. Which AWS service allows for centralized management of single sign-on (SSO) access?",
            "options": [
                "AWS Directory Service",
                "AWS Identity and Access Management (IAM)",
                "AWS Security Token Service (STS)",
                "AWS Single Sign-On (AWS SSO)"
            ],
            "answer": [3],
            "explanation": "AWS Single Sign-On (AWS SSO) allows you to centrally manage SSO access and user permissions across multiple AWS accounts."
        },
        {
            "question": "A company needs to design a VPC architecture where EC2 instances in private subnets require outbound internet access to download updates. Which combination of actions will achieve this securely? (Select TWO.)",
            "options": [
                "Create a NAT gateway in a public subnet.",
                "Assign Elastic IP addresses to the EC2 instances in private subnets.",
                "Create an internet gateway and attach it to the VPC.",
                "Update the route table of the private subnet to direct traffic to the NAT gateway.",
                "Update the route table of the public subnet to direct traffic to the NAT gateway."
            ],
            "answer": [0, 3],
            "explanation": "Creating a NAT gateway in a public subnet allows instances in private subnets to access the internet while remaining unreachable from the internet. The route table of the private subnet must be updated to direct traffic to the NAT gateway."
        },
        {
            "question": "A company's security policy requires that all data stored in Amazon S3 must be encrypted using keys that are managed on-premises. Which encryption method should the company use?",
            "options": [
                "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
                "Server-side encryption with AWS KMS managed keys (SSE-KMS)",
                "Server-side encryption with customer-provided keys (SSE-C)",
                "Client-side encryption"
            ],
            "answer": [2],
            "explanation": "Server-side encryption with customer-provided keys (SSE-C) allows you to manage your encryption keys on-premises and provide them to AWS for encrypting data at rest."
        },
        {
            "question": "A company needs to implement a resilient architecture for its application hosted on EC2 instances across multiple Availability Zones. Which services should the solutions architect recommend? (Select TWO.)",
            "options": [
                "Amazon Route 53 with weighted routing",
                "Elastic Load Balancer (ELB)",
                "Amazon RDS with Multi-AZ deployment",
                "AWS CloudTrail",
                "Amazon S3 with Cross-Region Replication"
            ],
            "answer": [1, 2],
            "explanation": "An Elastic Load Balancer (ELB) distributes incoming traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. Amazon RDS with Multi-AZ deployment provides enhanced availability and durability for database instances."
        },
        {
            "question": "A company needs to allow a VPC's private subnets to access AWS services without using public IP addresses. Which solution should the company implement?",
            "options": [
                "NAT Gateway",
                "VPC Peering",
                "AWS Direct Connect",
                "VPC Endpoint"
            ],
            "answer": [3],
            "explanation": "A VPC endpoint enables you to privately connect your VPC to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection."
        },
        {
            "question": "A company is using Amazon RDS for a high-traffic OLTP database. To improve read performance, they want to offload read traffic from the primary database. What solution should the company implement?",
            "options": [
                "Use Amazon ElastiCache",
                "Create Read Replicas",
                "Enable Multi-AZ deployment",
                "Use Amazon DynamoDB"
            ],
            "answer": [1],
            "explanation": "Creating Read Replicas allows you to offload read traffic from the primary database, improving read performance and reducing the load on the primary instance."
        },
        {
            "question": "A company is designing a data storage solution that optimizes cost for infrequently accessed data while maintaining rapid access when needed. Which Amazon S3 storage class should be used?",
            "options": [
                "S3 Standard",
                "S3 Intelligent-Tiering",
                "S3 Standard-IA",
                "S3 One Zone-IA"
            ],
            "answer": [2],
            "explanation": "S3 Standard-IA (Infrequent Access) is designed for data that is accessed less frequently but requires rapid access when needed. It offers lower storage costs compared to S3 Standard."
        },
        {
            "question": "A company's application needs to process a large number of transactions with minimal latency. The workload requires both high read and write throughput. Which AWS service is best suited for this requirement?",
            "options": [
                "Amazon Aurora",
                "Amazon Redshift",
                "Amazon DynamoDB",
                "Amazon RDS"
            ],
            "answer": [2],
            "explanation": "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to handle high throughput for both read and write operations."
        },
        {
            "question": "A company has a distributed microservices architecture using Amazon ECS. They need a solution to centralize logging from all the microservices. Which AWS service should they use?",
            "options": [
                "AWS CloudTrail",
                "Amazon CloudWatch Logs",
                "Amazon Kinesis",
                "AWS X-Ray"
            ],
            "answer": [1],
            "explanation": "Amazon CloudWatch Logs can be used to collect and store logs from your microservices running in Amazon ECS, providing centralized logging."
        },
        {
            "question": "A company wants to automate the deployment of their application to AWS, including launching EC2 instances, configuring security groups, and deploying application code. Which AWS service should they use?",
            "options": [
                "AWS CloudFormation",
                "AWS Elastic Beanstalk",
                "AWS CodeDeploy",
                "AWS OpsWorks"
            ],
            "answer": [1],
            "explanation": "AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. It handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring."
        },
        {
            "question": "A company is running a mission-critical application on a single Amazon EC2 instance. To ensure high availability, what should the solutions architect recommend?",
            "options": [
                "Enable EC2 Auto Scaling",
                "Create a second EC2 instance in the same Availability Zone",
                "Create an Amazon RDS Multi-AZ DB instance",
                "Deploy the application across multiple Availability Zones"
            ],
            "answer": [3],
            "explanation": "Deploying the application across multiple Availability Zones ensures high availability and fault tolerance, as it protects against a failure in a single Availability Zone."
        },
        {
            "question": "A company needs to securely store and manage access to its configuration files in AWS. Which service should they use to achieve this?",
            "options": [
                "Amazon S3",
                "AWS Systems Manager Parameter Store",
                "AWS Secrets Manager",
                "Amazon EFS"
            ],
            "answer": [1],
            "explanation": "AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management."
        },
        {
            "question": "A company needs to design a cost-optimized solution for storing petabytes of archive data that is infrequently accessed. Which Amazon S3 storage class should they choose?",
            "options": [
                "S3 Standard",
                "S3 Intelligent-Tiering",
                "S3 Glacier",
                "S3 One Zone-IA"
            ],
            "answer": [2],
            "explanation": "S3 Glacier is a secure, durable, and low-cost storage class for data archiving and long-term backup. It is designed for infrequently accessed data."
        },
        {
            "question": "A company needs to ensure compliance with their internal security policies by regularly auditing their AWS infrastructure. Which AWS service can provide a history of API calls made to their AWS account?",
            "options": [
                "AWS CloudTrail",
                "Amazon CloudWatch",
                "AWS Config",
                "AWS Trusted Advisor"
            ],
            "answer": [0],
            "explanation": "AWS CloudTrail records AWS API calls for your account and delivers log files to you, enabling governance, compliance, and operational and risk auditing of your AWS account."
        },
        {
            "question": "A company has multiple AWS accounts and wants to consolidate billing to take advantage of volume pricing discounts. Which AWS service should they use?",
            "options": [
                "AWS Organizations",
                "AWS Budgets",
                "AWS Cost Explorer",
                "AWS Trusted Advisor"
            ],
            "answer": [0],
            "explanation": "AWS Organizations allows you to consolidate multiple AWS accounts into an organization that you create and centrally manage, enabling you to benefit from volume pricing discounts."
        },
    {
        "question": "Which Amazon EC2 pricing model should be used for applications with steady-state usage?",
        "options": [
            "On-Demand Instances",
            "Spot Instances",
            "Reserved Instances",
            "Dedicated Hosts"
        ],
        "answer": [2],
        "explanation": "Reserved Instances are suitable for applications with steady-state usage as they offer significant cost savings over On-Demand pricing."
    },
    {
        "question": "Which AWS service can be used to run containerized applications without managing servers or clusters of Amazon EC2 instances?",
        "options": [
            "Amazon ECS",
            "AWS Lambda",
            "Amazon EKS",
            "AWS Fargate"
        ],
        "answer": [3],
        "explanation": "AWS Fargate is a serverless compute engine for containers that works with both Amazon ECS and Amazon EKS."
    },
    {
        "question": "A company needs to run a batch job on Amazon EC2 that can tolerate interruptions and requires cost optimization. Which EC2 instance type should be used? (Select TWO.)",
        "options": [
            "On-Demand Instances",
            "Spot Instances",
            "Reserved Instances",
            "Dedicated Hosts",
            "EC2 Fleet"
        ],
        "answer": [1, 4],
        "explanation": "Spot Instances offer cost savings and are suitable for batch jobs that can tolerate interruptions. EC2 Fleet allows you to use a combination of instance types and pricing options."
    },
    {
        "question": "Which features does Amazon EC2 Auto Scaling provide to help maintain application availability? (Select TWO.)",
        "options": [
            "Scaling horizontally by adding more instances",
            "Scaling vertically by increasing instance size",
            "Automatic scaling based on demand",
            "Managed databases",
            "Automated code deployments"
        ],
        "answer": [0, 2],
        "explanation": "Amazon EC2 Auto Scaling helps maintain application availability by automatically adding or removing instances based on demand."
    },
    {
        "question": "A company wants to ensure high availability for its web application hosted on Amazon EC2. Which options should they configure? (Select TWO.)",
        "options": [
            "Deploy instances across multiple Availability Zones",
            "Use Spot Instances",
            "Configure Auto Scaling groups",
            "Enable Multi-AZ for RDS",
            "Use Dedicated Hosts"
        ],
        "answer": [0, 2],
        "explanation": "Deploying instances across multiple Availability Zones and using Auto Scaling groups are key strategies for ensuring high availability."
    },
    {
        "question": "Which Amazon S3 storage class is designed for data that is accessed less frequently but requires rapid access when needed?",
        "options": [
            "S3 Standard",
            "S3 Intelligent-Tiering",
            "S3 Standard-IA",
            "S3 One Zone-IA"
        ],
        "answer": [2],
        "explanation": "S3 Standard-IA (Infrequent Access) is designed for data that is accessed less frequently but requires rapid access when needed."
    },
    {
        "question": "Which AWS service provides a fully managed file system that can be mounted by on-premises servers and AWS resources?",
        "options": [
            "Amazon S3",
            "Amazon EBS",
            "Amazon EFS",
            "AWS Storage Gateway"
        ],
        "answer": [3],
        "explanation": "AWS Storage Gateway provides a hybrid cloud storage service that enables on-premises applications to use AWS cloud storage."
    },
    {
        "question": "A company needs a storage solution for static website hosting. Which Amazon S3 features should they configure? (Select TWO.)",
        "options": [
            "Bucket policy to allow public read access",
            "S3 Object Lock",
            "Versioning",
            "Website hosting configuration",
            "Cross-Region Replication"
        ],
        "answer": [0, 3],
        "explanation": "To host a static website on Amazon S3, configure the bucket policy to allow public read access and enable the website hosting configuration."
    },
    {
        "question": "Which features does Amazon S3 provide to ensure data durability and availability? (Select TWO.)",
        "options": [
            "Automatic scaling",
            "Cross-Region Replication",
            "Multi-AZ storage",
            "Lifecycle policies",
            "Versioning"
        ],
        "answer": [2, 4],
        "explanation": "Amazon S3 ensures data durability and availability with Multi-AZ storage and versioning."
    },
    {
        "question": "A company needs to move large amounts of data from their on-premises data center to AWS quickly and securely. Which services can they use? (Select TWO.)",
        "options": [
            "AWS Snowball",
            "AWS DataSync",
            "Amazon S3 Transfer Acceleration",
            "AWS Direct Connect",
            "AWS Storage Gateway"
        ],
        "answer": [0, 1],
        "explanation": "AWS Snowball and AWS DataSync are designed for moving large amounts of data quickly and securely to AWS."
    },
    {
        "question": "Which Amazon RDS feature provides high availability and failover support for database instances?",
        "options": [
            "Read Replicas",
            "Multi-AZ Deployment",
            "Reserved Instances",
            "Automated Backups"
        ],
        "answer": [1],
        "explanation": "Multi-AZ Deployment provides high availability and failover support for Amazon RDS database instances."
    },
    {
        "question": "Which AWS service is a fully managed NoSQL database that provides fast and predictable performance with seamless scalability?",
        "options": [
            "Amazon RDS",
            "Amazon DynamoDB",
            "Amazon Redshift",
            "Amazon Aurora"
        ],
        "answer": [1],
        "explanation": "Amazon DynamoDB is a fully managed NoSQL database that provides fast and predictable performance with seamless scalability."
    },
    {
        "question": "A company needs to improve the read performance of their Amazon RDS database. Which options should they consider? (Select TWO.)",
        "options": [
            "Create Read Replicas",
            "Enable Multi-AZ deployment",
            "Increase instance size",
            "Enable automated backups",
            "Use provisioned IOPS storage"
        ],
        "answer": [0, 2],
        "explanation": "Creating Read Replicas and increasing the instance size can improve the read performance of an Amazon RDS database."
    },
    {
        "question": "Which features does Amazon Aurora provide to enhance database performance and availability? (Select TWO.)",
        "options": [
            "Global Databases",
            "Cross-Region Replication",
            "Parallel Query",
            "DAX (DynamoDB Accelerator)",
            "Automatic scaling"
        ],
        "answer": [0, 2],
        "explanation": "Amazon Aurora enhances database performance and availability with features like Global Databases and Parallel Query."
    },
    {
        "question": "A company is running an Amazon RDS database instance with a high read traffic load. Which solutions can help distribute the read traffic? (Select TWO.)",
        "options": [
            "Multi-AZ deployment",
            "Read Replicas",
            "Elastic Load Balancer",
            "Amazon ElastiCache",
            "Amazon CloudFront"
        ],
        "answer": [1, 3],
        "explanation": "Read Replicas and Amazon ElastiCache can help distribute the read traffic load for an Amazon RDS database instance."
    },
    {
        "question": "A company runs a large batch processing job at the end of every quarter. The processing job runs for 5 days and uses 15 Amazon EC2 instances. The processing must run uninterrupted for 5 hours per day. The company is investigating ways to reduce the cost of the batch processing job. Which pricing model should the company choose?",
        "options": [
            "On-Demand Instances",
            "Spot Instances",
            "Reserved Instances",
            "Dedicated Instances"
        ],
        "answer": [0],
        "explanation": "On-Demand Instances are suitable for jobs that need to run without interruption and have a variable schedule, such as this batch processing job that runs at the end of every quarter for 5 days. On-Demand Instances provide flexibility and do not require long-term commitments or upfront payments, making them ideal for workloads with unpredictable usage patterns."
    },
    {
        "question": "A new application is to be published in multiple regions around the world. The Architect needs to ensure only 2 IP addresses need to be whitelisted. The solution should intelligently route traffic for lowest latency and provide fast regional failover. How can this be achieved?",
        "options": [
            "Launch EC2 instances into multiple regions behind an NLB and use AWS Global Accelerator",
            "Launch EC2 instances into multiple regions behind an ALB and use a Route 53 failover routing policy",
            "Launch EC2 instances into multiple regions behind an NLB with a static IP address",
            "Launch EC2 instances into multiple regions behind an ALB and use Amazon CloudFront with a pair of static IP addresses"
        ],
        "answer": [0],
        "explanation": "AWS Global Accelerator uses the vast, congestion-free AWS global network to route TCP and UDP traffic to a healthy application endpoint in the closest AWS Region to the user. This means it will intelligently route traffic to the closest point of presence, reducing latency. Seamless failover is ensured as AWS Global Accelerator uses anycast IP addresses, which means the IP does not change when failing over between regions, so there are no issues with client caches having incorrect entries that need to expire. This is the only solution that provides deterministic failover."
    },
    {
        "question": "An Amazon S3 bucket in the us-east-1 Region hosts the static website content of a company. The content is made available through an Amazon CloudFront origin pointing to that bucket. A second copy of the bucket is created in the ap-southeast-1 Region using cross-region replication. The chief solutions architect wants a solution that provides greater availability for the website. Which combination of actions should a solutions architect take to increase availability? (Select TWO.)",
        "options": [
            "Using us-east-1 bucket as the primary bucket and ap-southeast-1 bucket as the secondary bucket, create a CloudFront origin group.",
            "Set up failover routing in Amazon Route 53.",
            "Point Amazon Route 53 to the replica bucket by creating a record.",
            "Create an origin for CloudFront for both buckets.",
            "Add an origin for ap-southeast-1 to CloudFront."
        ],
        "answer": [0, 4],
        "explanation": "Creating a CloudFront origin group with the us-east-1 bucket as the primary origin and the ap-southeast-1 bucket as the secondary origin ensures that CloudFront will automatically failover to the secondary origin if the primary origin becomes unavailable. Adding an origin for the ap-southeast-1 bucket to CloudFront ensures that content can be served from both regions, increasing availability."
    },
    {
        "question": "A company runs a dynamic website that is hosted on an on-premises server in the United States. The company is expanding to Europe and is investigating how they can optimize the performance of the website for European users. The websiteâ€™s backend must remain in the United States. The company requires a solution that can be implemented within a few days. What should a Solutions Architect recommend?",
        "options": [
            "Launch an Amazon EC2 instance in an AWS Region in the United States and migrate the website to it.",
            "Migrate the website to Amazon S3. Use cross-Region replication between Regions and a latency-based Route 53 policy.",
            "Use Amazon CloudFront with Lambda@Edge to direct traffic to an on-premises origin.",
            "Use Amazon CloudFront with a custom origin pointing to the on-premises servers."
        ],
        "answer": [3],
        "explanation": "Amazon CloudFront with a custom origin pointing to the on-premises servers is the optimal solution for improving performance for European users while keeping the backend in the United States. CloudFront will cache content at edge locations close to the European users, reducing latency and improving load times. This solution can be implemented quickly and does not require changes to the backend infrastructure."
    },
    {
        "question": "A company needs to connect its on-premises data center network to a new virtual private cloud (VPC). There is a symmetrical internet connection of 100 Mbps in the data center network. The data transfer rate for an on-premises application is multiple gigabytes per day. Processing will be done using an Amazon Kinesis Data Firehose stream. What should a solutions architect recommend for maximum performance?",
        "options": [
            "Kinesis Data Firehose can be connected to the VPC using AWS PrivateLink. Install a 1 Gbps AWS Direct Connect connection between the on-premises network and AWS. To send data from on-premises to Kinesis Data Firehose, use the PrivateLink endpoint.",
            "Get an AWS Snowball Edge Storage Optimized device. Data must be copied to the device after several days and shipped to AWS for expedited transfer to Kinesis Data Firehose. Repeat as necessary.",
            "Establish a peering connection between the on-premises network and the VPC. Configure routing for the on-premises network to use the VPC peering connection.",
            "Establish an AWS Site-to-Site VPN connection between the on-premises network and the VPC. Set up BGP routing between the customer gateway and the virtual private gateway. Send data to Kinesis Data Firehose using a VPN connection."
        ],
        "answer": [0],
        "explanation": "To achieve maximum performance for data transfer, the best option is to install a 1 Gbps AWS Direct Connect connection between the on-premises network and AWS. This provides a high-bandwidth, low-latency, and reliable connection. Using AWS PrivateLink to connect Kinesis Data Firehose to the VPC ensures secure and private communication, bypassing the internet and enhancing performance and security."
    },
    {
        "question": "A legacy tightly-coupled High Performance Computing (HPC) application will be migrated to AWS. Which network adapter type should be used?",
        "options": [
            "Elastic Network Interface (ENI)",
            "Elastic Fabric Adapter (EFA)",
            "Elastic Network Adapter (ENA)",
            "Elastic IP Address"
        ],
        "answer": [1],
        "explanation": "Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. EFA provides lower and more consistent latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems. It is specifically designed for tightly-coupled HPC applications that require high levels of inter-node communication, making it the ideal choice for migrating such applications to AWS."
    },
    {
        "question": "The database tier of a web application is running on a Windows server on-premises. The database is a Microsoft SQL Server database. The application owner would like to migrate the database to an Amazon RDS instance. How can the migration be executed with minimal administrative effort and downtime?",
        "options": [
            "Use AWS DataSync to migrate the data from the database to Amazon S3. Use AWS Database Migration Service (DMS) to migrate the database to RDS.",
            "Use the AWS Server Migration Service (SMS) to migrate the server to Amazon EC2. Use AWS Database Migration Service (DMS) to migrate the database to RDS.",
            "Use the AWS Database Migration Service (DMS) to directly migrate the database to RDS. Use the Schema Conversion Tool (SCT) to enable conversion from Microsoft SQL Server to Amazon RDS.",
            "Use the AWS Database Migration Service (DMS) to directly migrate the database to RDS."
        ],
        "answer": [3],
        "explanation": "The AWS Database Migration Service (DMS) is specifically designed to migrate databases to AWS with minimal downtime. DMS supports continuous data replication, which allows the database to remain operational during the migration process, minimizing downtime. Since the source database is Microsoft SQL Server and the target is Amazon RDS for SQL Server, there is no need for schema conversion, making this a straightforward migration."
    },
    {
        "question": "A video production company is planning to move some of its workloads to the AWS Cloud. The company will require around 5 TB of storage for video processing with the maximum possible I/O performance. They also require over 400 TB of extremely durable storage for storing video files and 800 TB of storage for long-term archival. Which combinations of services should a Solutions Architect use to meet these requirements?",
        "options": [
            "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.",
            "Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage.",
            "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.",
            "Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage."
        ],
        "answer": [2],
        "explanation": "Amazon EC2 instance store provides the highest possible I/O performance for temporary storage, which is ideal for video processing workloads that require high performance. Amazon S3 is designed for high durability and scalability, making it suitable for storing large amounts of video files. Amazon S3 Glacier is designed for long-term archival storage with high durability and low cost, making it ideal for storing large volumes of data that are infrequently accessed."
    },
    {
        "question": "An Amazon RDS Read Replica is being deployed in a separate region. The master database is not encrypted but all data in the new region must be encrypted. How can this be achieved?",
        "options": [
            "Encrypt a snapshot from the master DB instance, create a new encrypted master DB instance, and then create an encrypted cross-region Read Replica.",
            "Enable encryption using Key Management Service (KMS) when creating the cross-region Read Replica.",
            "Encrypt a snapshot from the master DB instance, create an encrypted cross-region Read Replica from the snapshot.",
            "Enable encryption on the master DB instance, then create an encrypted cross-region Read Replica."
        ],
        "answer": [0],
        "explanation": "To achieve encryption for the new region's data, you must first encrypt a snapshot from the master DB instance. Then, create a new encrypted master DB instance from that snapshot. After the new encrypted master DB instance is created, you can then create an encrypted cross-region Read Replica from the new encrypted master DB instance. This ensures that all data in the new region is encrypted, as required."
    },
    {
        "question": "A web application allows users to upload photos and add graphical elements to them. The application offers two tiers of service: free and paid. Photos uploaded by paid users should be processed before those submitted using the free tier. The photos are uploaded to an Amazon S3 bucket which uses an event notification to send the job information to Amazon SQS. How should a Solutions Architect configure the Amazon SQS deployment to meet these requirements?",
        "options": [
            "Use one SQS standard queue. Use batching for the paid photos and short polling for the free photos.",
            "Use a separate SQS Standard queue for each tier. Configure Amazon EC2 instances to prioritize polling for the paid queue over the free queue.",
            "Use a separate SQS FIFO queue for each tier. Set the free queue to use short polling and the paid queue to use long polling.",
            "Use one SQS FIFO queue. Assign a higher priority to the paid photos so they are processed first."
        ],
        "answer": [1],
        "explanation": "To ensure that photos uploaded by paid users are processed before those submitted by free users, you should use separate SQS Standard queues for each tier. By configuring Amazon EC2 instances to prioritize polling for the paid queue over the free queue, you ensure that paid user jobs are processed first. This configuration allows for flexibility and ensures that the priority of job processing is maintained without the limitations of FIFO queues, which are not necessary in this case."
    },
    {
        "question": "A company uses an Amazon RDS MySQL database instance to store customer order data. The security team have requested that SSL/TLS encryption in transit must be used for encrypting connections to the database from application servers. The data in the database is currently encrypted at rest using an AWS KMS key. How can a Solutions Architect enable encryption in transit?",
        "options": [
            "Download the AWS-provided root certificates. Use the certificates when connecting to the RDS DB instance.",
            "Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption in transit enabled.",
            "Add a self-signed certificate to the RDS DB instance. Use the certificates in all connections to the RDS DB instance.",
            "Enable encryption in transit using the RDS Management console and obtain a key using AWS KMS."
        ],
        "answer": [0],
        "explanation": "To enable SSL/TLS encryption in transit for an Amazon RDS MySQL database, you need to download the AWS-provided root certificates and configure your database clients to use these certificates when connecting to the RDS DB instance. This ensures that all connections to the database are encrypted in transit."
    },
    {
        "question": "A surveying team is using a fleet of drones to collect images of construction sites. The surveying team's laptops lack the inbuilt storage and compute capacity to transfer the images and process the data. While the team has Amazon EC2 instances for processing and Amazon S3 buckets for storage, network connectivity is intermittent and unreliable. The images need to be processed to evaluate the progress of each construction site. What should a solutions architect recommend?",
        "options": [
            "Process and store the images using AWS Snowball Edge devices.",
            "Cache the images locally on a hardware appliance pre-installed with AWS Storage Gateway to process the images when connectivity is restored.",
            "During intermittent connectivity to EC2 instances, upload images to Amazon SQS.",
            "Configure Amazon Kinesis Data Firehose to create multiple delivery streams aimed separately at the S3 buckets for storage and the EC2 instances for processing the images."
        ],
        "answer": [0],
        "explanation": "AWS Snowball Edge devices are ideal for environments with intermittent or unreliable network connectivity. They provide local storage and compute capacity, allowing the surveying team to transfer and process the images on-site. Once the images are processed, the data can be physically transported to AWS, where it can be further stored in Amazon S3 and processed by Amazon EC2 instances. This approach mitigates the issues caused by unreliable network connectivity and provides both storage and compute resources on-site."
    },
    {
        "question": "A team are planning to run analytics jobs on log files each day and require a storage solution. The size and number of logs is unknown and data will persist for 24 hours only. What is the MOST cost-effective solution?",
        "options": [
            "Amazon S3 Glacier Deep Archive",
            "Amazon S3 Intelligent-Tiering",
            "Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)",
            "Amazon S3 Standard"
        ],
        "answer": [3],
        "explanation": "Amazon S3 Standard is the most cost-effective solution for data that is frequently accessed and needs to persist for a short period of time, such as 24 hours. It is designed for high durability and availability, and it does not incur additional costs for frequent access or early deletion, making it ideal for the scenario where log files are analyzed daily and then discarded."
    },
    {
        "question": "A company hosts an application on Amazon EC2 instances behind Application Load Balancers in several AWS Regions. Distribution rights for the content require that users in different geographies must be served content from specific regions. Which configuration meets these requirements?",
        "options": [
            "Configure Application Load Balancers with multi-Region routing.",
            "Configure Amazon CloudFront with multiple origins and AWS WAF.",
            "Create Amazon Route 53 records with a geoproximity routing policy.",
            "Create Amazon Route 53 records with a geolocation routing policy."
        ],
        "answer": [3],
        "explanation": "Amazon Route 53 geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users. This is the ideal solution for meeting distribution rights requirements that mandate serving content from specific regions based on user location. By using geolocation routing, you can direct traffic to the appropriate regional Application Load Balancers."
    },
    {
        "question": "A solutions architect is creating a system that will run analytics on financial data for several hours a night, 5 days a week. The analysis is expected to run for the same duration and cannot be interrupted once it is started. The system will be required for a minimum of 1 year. What should the solutions architect configure to ensure the EC2 instances are available when they are needed?",
        "options": [
            "Savings Plans",
            "On-Demand Capacity Reservations",
            "Regional Reserved Instances",
            "On-Demand Instances"
        ],
        "answer": [1],
        "explanation": "On-Demand Capacity Reservations ensure that you have access to the EC2 capacity you need when you need it. They are designed to ensure the availability of instances in specific Availability Zones, making them suitable for workloads that require guaranteed capacity. This is ideal for the financial data analysis that needs to run without interruption for a set duration and frequency."
    },
    {
        "question": "A developer created an application that uses Amazon EC2 and an Amazon RDS MySQL database instance. The developer stored the database user name and password in a configuration file on the root EBS volume of the EC2 application instance. A Solutions Architect has been asked to design a more secure solution. What should the Solutions Architect do to achieve this requirement?",
        "options": [
            "Move the configuration file to an Amazon S3 bucket. Create an IAM role with permission to the bucket and attach it to the EC2 instance.",
            "Install an Amazon-trusted root certificate on the application instance and use SSL/TLS encrypted connections to the database.",
            "Attach an additional volume to the EC2 instance with encryption enabled. Move the configuration file to the encrypted volume.",
            "Create an IAM role with permission to access the database. Attach this IAM role to the EC2 instance."
        ],
        "answer": [3],
        "explanation": "Storing sensitive information like database credentials in a configuration file on an EC2 instance's root EBS volume is not secure. The best practice is to use IAM roles to grant the necessary permissions to the EC2 instance. This way, the application can securely access the database without needing to store credentials in a configuration file. IAM roles provide temporary security credentials and eliminate the need for hardcoding sensitive information."
    },
    {
        "question": "A company's application is running on Amazon EC2 instances in a single Region. In the event of a disaster, a solutions architect needs to ensure that the resources can also be deployed to a second Region. Which combination of actions should the solutions architect take to accomplish this? (Select TWO.)",
        "options": [
            "Launch a new EC2 instance in the second Region and copy a volume from Amazon S3 to the new instance.",
            "Copy an Amazon Machine Image (AMI) of an EC2 instance and specify the second Region for the destination.",
            "Detach a volume on an EC2 instance and copy it to an Amazon S3 bucket in the second Region.",
            "Launch a new EC2 instance from an Amazon Machine Image (AMI) in the second Region.",
            "Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2 instance in the second Region using that EBS volume."
        ],
        "answer": [1, 3],
        "explanation": "To ensure that resources can be deployed to a second Region in the event of a disaster, the best approach is to create an Amazon Machine Image (AMI) of your existing EC2 instances and copy the AMI to the second Region. Once the AMI is available in the second Region, you can launch new EC2 instances from the AMI, ensuring that the application can be quickly deployed in the new Region."
    },
    {
        "question": "A company requires that all AWS IAM user accounts have specific complexity requirements and minimum password length. How should a Solutions Architect accomplish this?",
        "options": [
            "Use an AWS Config rule to enforce the requirements when creating user accounts.",
            "Set a password policy for each IAM user in the AWS account.",
            "Set a password policy for the entire AWS account.",
            "Create an IAM policy that enforces the requirements and apply it to all users."
        ],
        "answer": [2],
        "explanation": "To enforce specific complexity requirements and minimum password length for all IAM user accounts, you should set a password policy for the entire AWS account. This password policy will apply to all users within the account and enforce the desired security standards, such as complexity requirements and minimum length."
    },
    {
        "question": "Amazon EC2 instances in a development environment run between 9am and 5pm Monday-Friday. Production instances run 24/7. Which pricing models should be used to optimize cost and ensure capacity is available? (Select TWO.)",
        "options": [
            "On-demand capacity reservations for the development environment",
            "Use On-Demand instances for the production environment",
            "Use Spot instances for the development environment",
            "Use Reserved instances for the development environment",
            "Use Reserved instances for the production environment"
        ],
        "answer": [0, 4],
        "explanation": "On-demand capacity reservations for the development environment: Since the development environment has predictable usage patterns (9am to 5pm, Monday to Friday), On-demand capacity reservations ensure that capacity is available during those hours without long-term commitments, optimizing costs. Reserved instances for the production environment: Production instances run 24/7, making them ideal for Reserved instances. Reserved instances provide significant cost savings over On-Demand instances for workloads with predictable, steady-state usage."
    },
    {
        "question": "A Solutions Architect has deployed an application on several Amazon EC2 instances across three private subnets. The application must be made accessible to internet-based clients with the least amount of administrative effort. How can the Solutions Architect make the application available on the internet?",
        "options": [
            "Create an Application Load Balancer and associate three private subnets from the same Availability Zones as the private instances. Add the private instances to the ALB.",
            "Create an Application Load Balancer and associate three public subnets from the same Availability Zones as the private instances. Add the private instances to the ALB.",
            "Create a NAT gateway in a public subnet. Add a route to the NAT gateway to the route tables of the three private subnets.",
            "Create an Amazon Machine Image (AMI) of the instances in the private subnet and launch new instances from the AMI in public subnets. Create an Application Load Balancer and add the public instances to the ALB."
        ],
        "answer": [1],
        "explanation": "To make the application accessible to internet-based clients, you should create an Application Load Balancer (ALB) and associate it with public subnets in the same Availability Zones as the private instances. This setup ensures that the ALB has public IP addresses and can route traffic from the internet to the private instances, which remain secure in private subnets. The ALB will forward the traffic to the instances in the private subnets, making the application available to external clients with minimal administrative effort."
    },
    {
        "question": "An application running on an Amazon ECS container instance using the EC2 launch type needs permissions to write data to Amazon DynamoDB. How can you assign these permissions only to the specific ECS task that is running the application?",
        "options": [
            "Modify the AmazonECSTaskExecutionRolePolicy policy to add permissions for DynamoDB.",
            "Create an IAM policy with permissions to DynamoDB and assign it to a task using the taskRoleArn parameter.",
            "Use a security group to allow outbound connections to DynamoDB and assign it to the container instance.",
            "Create an IAM policy with permissions to DynamoDB and attach it to the container instance."
        ],
        "answer": [1],
        "explanation": "To assign permissions to a specific ECS task, you should create an IAM policy with the required permissions for DynamoDB and assign it to the ECS task using the `taskRoleArn` parameter. This ensures that only the specific task running the application has the necessary permissions to write data to DynamoDB, following the principle of least privilege."
    },
    {
        "question": "An eCommerce application consists of three tiers. The web tier includes EC2 instances behind an Application Load balancer, the middle tier uses EC2 instances and an Amazon SQS queue to process orders, and the database tier consists of an Auto Scaling DynamoDB table. During busy periods customers have complained about delays in the processing of orders. A Solutions Architect has been tasked with reducing processing times. Which action will be MOST effective in accomplishing this requirement?",
        "options": [
            "Use Amazon EC2 Auto Scaling to scale out the middle tier instances based on the SQS queue depth.",
            "Replace the Amazon SQS queue with Amazon Kinesis Data Firehose.",
            "Use Amazon DynamoDB Accelerator (DAX) in front of the DynamoDB backend tier.",
            "Add an Amazon CloudFront distribution with a custom origin to cache the responses for the web tier."
        ],
        "answer": [0],
        "explanation": "To address delays in order processing during busy periods, the most effective solution is to use Amazon EC2 Auto Scaling to scale out the middle tier instances based on the depth of the Amazon SQS queue. This ensures that additional EC2 instances are launched to handle the increased load and process the queued orders more quickly, reducing delays and improving performance."
    }
,
    {
        "question": "A company provides a REST-based interface to an application that allows a partner company to send data in near-real time. The application then processes the data that is received and stores it for later analysis. The application runs on Amazon EC2 instances.\n\nThe partner company has received many 503 Service Unavailable Errors when sending data to the application and the compute capacity reaches its limits and is unable to process requests when spikes in data volume occur.\n\nWhich design should a Solutions Architect implement to improve scalability?",
        "options": [
            "Use Amazon API Gateway in front of the existing application. Create a usage plan with a quota limit for the partner company.",
            "Use Amazon SQS to ingest the data. Configure the EC2 instances to process messages from the SQS queue.",
            "Use Amazon SNS to ingest the data and trigger AWS Lambda functions to process the data in near-real time.",
            "Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions."
        ],
        "answer": [3],
        "explanation": "Using Amazon Kinesis Data Streams to ingest the data allows for handling high throughput and processing data in near-real time. AWS Lambda can be triggered to process the data, providing a scalable and serverless solution that can handle spikes in data volume without causing 503 Service Unavailable errors."
    },
    {
        "question": "A company runs a web application that serves weather updates. The application runs on a fleet of Amazon EC2 instances in a Multi-AZ Auto scaling group behind an Application Load Balancer (ALB). The instances store data in an Amazon Aurora database. A solutions architect needs to make the application more resilient to sporadic increases in request rates.\n\nWhich architecture should the solutions architect implement? (Select TWO.)",
        "options": [
            "Add an AWS Transit Gateway to the Availability Zones",
            "Add an AWS Global Accelerator endpoint",
            "Add Amazon Aurora Replicas",
            "Add an Amazon CloudFront distribution in front of the ALB",
            "Add an AWS WAF in front of the ALB"
        ],
        "answer": [1, 2],
        "explanation": "Adding an AWS Global Accelerator endpoint can improve the availability and performance of the application by providing global static IP addresses and intelligently routing traffic to the optimal AWS endpoint based on performance, reducing latency and improving user experience during traffic spikes. Adding Amazon Aurora Replicas enhances the read scalability and availability of the Aurora database. This helps in distributing the read workload across multiple replicas, thereby improving performance during spikes in request rates."
    },
    {
        "question": "A web application runs in public and private subnets. The application architecture consists of a web tier and database tier running on Amazon EC2 instances. Both tiers run in a single Availability Zone (AZ).\n\nWhich combination of steps should a solutions architect take to provide high availability for this architecture? (Select TWO.)",
        "options": [
            "Create new public and private subnets in the same AZ for high availability",
            "Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)",
            "Create new public and private subnets in the same VPC, each in a new AZ. Migrate the database to an Amazon RDS multi-AZ deployment",
            "Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs",
            "Create new public and private subnets in a new AZ. Create a database using Amazon EC2 in one AZ"
        ],
        "answer": [2, 3],
        "explanation": "Creating new public and private subnets in different Availability Zones and migrating the database to an Amazon RDS multi-AZ deployment ensures that the database is highly available and fault-tolerant. Creating an EC2 Auto Scaling group and an Application Load Balancer (ALB) spanning multiple AZs ensures that the web application tier can handle increases in traffic and remain available even if one AZ goes down."
    },
    {
        "question": "An application is being created that will use Amazon EC2 instances to generate and store data. Another set of EC2 instances will then analyze and modify the data. Storage requirements will be significant and will continue to grow over time. The application architects require a storage solution.\n\nWhich actions would meet these needs?",
        "options": [
            "Store the data in Amazon S3 Glacier. Update the vault policy to allow access to the application instances",
            "Store the data in an Amazon EFS filesystem. Mount the file system on the application instances",
            "Store the data in an Amazon EBS volume. Mount the EBS volume on the application instances",
            "Store the data in AWS Storage Gateway. Setup AWS Direct Connect between the Gateway appliance and the EC2 instances"
        ],
        "answer": [1],
        "explanation": "Amazon EFS (Elastic File System) is a scalable file storage solution that can be mounted on multiple EC2 instances, providing a shared file system with high throughput and low latency. It is ideal for applications with significant and growing storage requirements, allowing seamless access and modification of data across multiple instances."
    },
    {
        "question": "An eCommerce company runs an application on Amazon EC2 instances in public and private subnets. The web application runs in a public subnet and the database runs in a private subnet. Both the public and private subnets are in a single Availability Zone.\n\nWhich combination of steps should a solutions architect take to provide high availability for this architecture? (Select TWO.)",
        "options": [
            "Create new public and private subnets in the same AZ but in a different Amazon VPC.",
            "Create an EC2 Auto Scaling group and Application Load Balancer that spans across multiple AZs.",
            "Create new public and private subnets in a different AZ. Create a database using Amazon EC2 in one AZ.",
            "Create an EC2 Auto Scaling group in the public subnet and use an Application Load Balancer.",
            "Create new public and private subnets in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment."
        ],
        "answer": [1, 4],
        "explanation": "Creating an EC2 Auto Scaling group and Application Load Balancer (ALB) that spans multiple Availability Zones ensures that the web tier can handle increased traffic and remain available even if one AZ goes down. Creating new public and private subnets in a different AZ and migrating the database to an Amazon RDS multi-AZ deployment ensures that the database is highly available and fault-tolerant."
    },
    {
        "question": "A company hosts a multiplayer game on AWS. The application uses Amazon EC2 instances in a single Availability Zone and users connect over Layer 4. Solutions Architect has been tasked with making the architecture highly available and also more cost-effective.\n\nHow can the solutions architect best meet these requirements? (Select TWO.)",
        "options": [
            "Increase the number of instances and use smaller EC2 instance types",
            "Configure an Auto Scaling group to add or remove instances in multiple Availability Zones automatically",
            "Configure an Application Load Balancer in front of the EC2 instances",
            "Configure an Auto Scaling group to add or remove instances in the Availability Zone automatically",
            "Configure a Network Load Balancer in front of the EC2 instances"
        ],
        "answer": [1, 4],
        "explanation": "Configuring an Auto Scaling group to add or remove instances in multiple Availability Zones automatically ensures high availability and cost-effectiveness by scaling the number of instances up or down based on demand and distributing them across multiple Availability Zones to avoid a single point of failure. Configuring a Network Load Balancer (NLB) in front of the EC2 instances is ideal for Layer 4 load balancing and ensures high availability by distributing incoming traffic across multiple instances in multiple Availability Zones."
    },
    {
        "question": "A company wishes to restrict access to their Amazon DynamoDB table to specific, private source IP addresses from their VPC. What should be done to secure access to the table?",
        "options": [
            "Create a gateway VPC endpoint and add an entry to the route table",
            "Create an interface VPC endpoint in the VPC with an Elastic Network Interface (ENI)",
            "Create the Amazon DynamoDB table in the VPC",
            "Create an AWS VPN connection to the Amazon DynamoDB endpoint"
        ],
        "answer": [0],
        "explanation": "Creating a gateway VPC endpoint for DynamoDB allows you to securely connect your VPC to DynamoDB without needing a public IP address, ensuring that all traffic between your VPC and DynamoDB stays within the AWS network. By adding an entry to the route table, you can control which subnets have access to the DynamoDB table, thereby restricting access to specific private IP addresses within your VPC."
    },
    {
        "question": "A company offers an online product brochure that is delivered from a static website running on Amazon S3. The companyâ€™s customers are mainly in the United States, Canada, and Europe. The company is looking to cost-effectively reduce the latency for users in these regions.\n\nWhat is the most cost-effective solution to these requirements?",
        "options": [
            "Create an Amazon CloudFront distribution that uses origins in U.S, Canada and Europe.",
            "Create an Amazon CloudFront distribution and set the price class to use only U.S, Canada and Europe.",
            "Create an Amazon CloudFront distribution and set the price class to use all Edge Locations for best performance.",
            "Create an Amazon CloudFront distribution and use Lambda@Edge to run the website's data processing closer to the users."
        ],
        "answer": [1],
        "explanation": "Creating an Amazon CloudFront distribution and setting the price class to use only U.S, Canada, and Europe is a cost-effective solution that reduces latency by using edge locations close to the customers in these regions. This approach balances performance and cost by limiting the edge locations to those regions where the majority of the company's customers are located."
    },
    {
        "question": "A company runs an application in an on-premises data center that collects environmental data from production machinery. The data consists of JSON files stored on network attached storage (NAS) and around 5 TB of data is collected each day. The company must upload this data to Amazon S3 where it can be processed by an analytics application. The data must be transferred securely.\n\nWhich solution offers the MOST reliable and time-efficient data transfer?",
        "options": [
            "Multiple AWS Snowcone devices.",
            "AWS DataSync over AWS Direct Connect.",
            "AWS Database Migration Service over the Internet.",
            "Amazon S3 Transfer Acceleration over the Internet."
        ],
        "answer": [1],
        "explanation": "AWS DataSync is a managed service that simplifies, automates, and accelerates moving data between on-premises storage and AWS. When combined with AWS Direct Connect, it provides a reliable, secure, and high-speed connection directly to AWS, ensuring efficient and secure data transfer. This combination is ideal for transferring large volumes of data like the 5 TB collected daily, ensuring minimal transfer times and high reliability."
    },
    {
        "question": "A new application will run across multiple Amazon ECS tasks. Front-end application logic will process data and then pass that data to a back-end ECS task to perform further processing and write the data to a datastore. The Architect would like to reduce-interdependencies so failures do not impact other components.\n\nWhich solution should the Architect use?",
        "options": [
            "Create an Amazon Kinesis Firehose delivery stream that delivers data to an Amazon S3 bucket, configure the front-end to write data to the stream and the back-end to read data from Amazon S3",
            "Create an Amazon SQS queue and configure the front-end to add messages to the queue and the back-end to poll the queue for messages",
            "Create an Amazon SQS queue that pushes messages to the back-end. Configure the front-end to add messages to the queue",
            "Create an Amazon Kinesis Firehose delivery stream and configure the front-end to add data to the stream and the back-end to read data from the stream"
        ],
        "answer": [1],
        "explanation": "Using Amazon SQS (Simple Queue Service) allows for decoupling the front-end and back-end ECS tasks, ensuring that failures in one component do not impact the other. The front-end can add messages to the queue, and the back-end can poll the queue for messages to process. This design reduces interdependencies and increases fault tolerance."
    },
    {
        "question": "A retail company with many stores and warehouses is implementing IoT sensors to gather monitoring data from devices in each location. The data will be sent to AWS in real time. A solutions architect must provide a solution for ensuring events are received in order for each device and ensure that data is saved for future processing.\n\nWhich solution would be MOST efficient?",
        "options": [
            "Use an Amazon SQS standard queue for real-time events with one queue for each device. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3",
            "Use an Amazon SQS FIFO queue for real-time events with one queue for each device. Trigger an AWS Lambda function for the SQS queue to save data to Amazon EFS",
            "Use Amazon Kinesis Data Streams for real-time events with a partition key for each device. Use Amazon Kinesis Data Firehose to save data to Amazon S3",
            "Use Amazon Kinesis Data Streams for real-time events with a shard for each device. Use Amazon Kinesis Data Firehose to save data to Amazon EBS"
        ],
        "answer": [2],
        "explanation": "Amazon Kinesis Data Streams is designed for real-time data ingestion and processing, and using a partition key for each device ensures that events are received in order for each device. Kinesis Data Firehose can then be used to efficiently deliver the data to Amazon S3 for long-term storage and future processing. This setup is both efficient and scalable for handling real-time IoT data from many devices."
    },
    {
        "question": "A company is working with a strategic partner that has an application that must be able to send messages to one of the companyâ€™s Amazon SQS queues. The partner company has its own AWS account.\n\nHow can a Solutions Architect provide least privilege access to the partner?",
        "options": [
            "Create a user account and grant the sqs:SendMessage permission for Amazon SQS. Share the credentials with the partner company.",
            "Create a cross-account role with access to all SQS queues and use the partner's AWS account in the trust document for the role.",
            "Update the permission policy on the SQS queue to grant all permissions to the partnerâ€™s AWS account.",
            "Update the permission policy on the SQS queue to grant the sqs:SendMessage permission to the partnerâ€™s AWS account."
        ],
        "answer": [3],
        "explanation": "The most secure and least privilege method to allow the partner to send messages to the SQS queue is to update the permission policy on the SQS queue itself to grant only the necessary permission (sqs:SendMessage) to the partner's AWS account. This approach ensures that the partner has only the required access and no additional permissions."
    }
]
